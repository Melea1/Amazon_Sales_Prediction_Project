{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "#!pip install pandas\n",
        "import pandas as pd\n",
        "#!pip install numpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "6UYqMHewH5Ja"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1 ‚Äî Load all data sources"
      ],
      "metadata": {
        "id": "XeAAfK1fIIY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all CSV files\n",
        "amazon = pd.read_csv('/content/Amazon Sale Report.csv')\n",
        "international = pd.read_csv('/content/International sale Report.csv')\n",
        "may = pd.read_csv('/content/May-2022.csv')\n",
        "sales = pd.read_csv('/content/Sale Report.csv')\n",
        "expense = pd.read_csv('/content/Expense IIGF.csv')\n",
        "\n",
        "print(\"Shapes:\",\n",
        "      \"\\nAmazon:\", amazon.shape,\n",
        "      \"\\nInternational:\", international.shape,\n",
        "      \"\\nMay:\", may.shape,\n",
        "      \"\\nSales:\", sales.shape,\n",
        "      \"\\nExpense:\", expense.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBzpcU2gH7jF",
        "outputId": "31c90108-cffd-4147-814b-911ab1f3d4f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: \n",
            "Amazon: (128975, 24) \n",
            "International: (37432, 10) \n",
            "May: (1330, 17) \n",
            "Sales: (9271, 7) \n",
            "Expense: (17, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2 ‚Äî Normalize column names & build the unified sales dataset"
      ],
      "metadata": {
        "id": "9zf2LSwmIGsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_columns(df):\n",
        "    out = df.copy()\n",
        "    out.columns = out.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    return out\n",
        "\n",
        "amazon = normalize_columns(amazon)\n",
        "international = normalize_columns(international)\n",
        "may = normalize_columns(may)\n",
        "sales = normalize_columns(sales)\n",
        "\n",
        "# Combine all sales data into one \"flat file\"\n",
        "merged = pd.concat([amazon, international, may, sales], axis=0, ignore_index=True)\n",
        "before = len(merged)\n",
        "merged = merged.drop_duplicates()\n",
        "print(f\"Dropped {before - len(merged)} duplicate rows\")\n",
        "print(\"Unified sales dataset shape:\", merged.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARaCYnPJIMny",
        "outputId": "efbbd24d-9dac-4bf7-e2dd-41180025f0eb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 0 duplicate rows\n",
            "Unified sales dataset shape: (177008, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3 ‚Äî Text cleaning, date conversion, and time features"
      ],
      "metadata": {
        "id": "cfjjBV1oIWWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(x):\n",
        "    if isinstance(x, str):\n",
        "        x = x.lower()\n",
        "        x = re.sub(r'[^a-z0-9 ]+', ' ', x)\n",
        "        x = \" \".join(x.split())\n",
        "    return x\n",
        "\n",
        "# Clean all text columns\n",
        "text_cols = merged.select_dtypes(include='object').columns\n",
        "for col in text_cols:\n",
        "    merged[col] = merged[col].astype(str).apply(clean_text)\n",
        "\n",
        "# Convert date columns to datetime\n",
        "for col in merged.columns:\n",
        "    if 'date' in col:\n",
        "        merged[col] = pd.to_datetime(merged[col], errors='coerce')\n",
        "\n",
        "# Create time-based features\n",
        "if 'date' in merged.columns:\n",
        "    merged['year'] = merged['date'].dt.year\n",
        "    merged['month'] = merged['date'].dt.month\n",
        "    merged['weekday'] = merged['date'].dt.weekday\n",
        "    merged['is_weekend'] = merged['weekday'].isin([5,6]).astype(int)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No main 'date' column detected ‚Äì cannot derive year/month features.\")"
      ],
      "metadata": {
        "id": "xbIDGSPIIUNq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4 ‚Äî Category simplification & missing value handling"
      ],
      "metadata": {
        "id": "2M2FLo7KIfpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce rare categories if 'category' exists\n",
        "if 'category' in merged.columns:\n",
        "    counts = merged['category'].value_counts()\n",
        "    rare = counts[counts < 20].index\n",
        "    merged['category'] = merged['category'].replace(rare, 'other')\n",
        "\n",
        "# Safe imputation (no row deletion)\n",
        "for col in merged.columns:\n",
        "    if merged[col].isna().any():\n",
        "        if merged[col].dtype == 'object':\n",
        "            merged[col] = merged[col].fillna('unknown')\n",
        "        else:\n",
        "            merged[col] = merged[col].fillna(merged[col].median())\n",
        "\n",
        "# Ensure quantity columns are numeric (useful for later ratios)\n",
        "for q in ['qty','pcs']:\n",
        "    if q in merged.columns:\n",
        "        merged[q] = pd.to_numeric(merged[q], errors='coerce')\n",
        "\n",
        "\n",
        "missing_percent = merged.isnull().mean() * 100\n",
        "high_missing_cols = missing_percent[missing_percent > 80].index.tolist()\n",
        "\n",
        "print(\"‚ö†Ô∏è Columns with >80% missing values:\", len(high_missing_cols))\n",
        "for c in high_missing_cols[:20]:\n",
        "    print(f\" - {c}: {missing_percent[c]:.2f}%\")\n",
        "print(\"Dataset shape (no deletion):\", merged.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4lInsQ8IfTd",
        "outputId": "e3cc6054-3658-4dca-9160-f51fd1cff7f2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Columns with >80% missing values: 1\n",
            " - pcs: 100.00%\n",
            "Dataset shape (no deletion): (177008, 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5 ‚Äî Expense dataset: cleaning, monthly aggregation, and merge"
      ],
      "metadata": {
        "id": "xRxmol5dIlWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: drop a stray 'index' column\n",
        "if 'index' in expense.columns:\n",
        "    expense = expense.drop(columns=['index'])\n",
        "\n",
        "print(\"Expense headers BEFORE handling:\", expense.columns.tolist())\n",
        "\n",
        "# Detect if already clean\n",
        "already_clean = {'raw_date','amount','description','note'}.issubset(expense.columns)\n",
        "\n",
        "if not already_clean:\n",
        "    # Try raw headers or normalized variants\n",
        "    rename_map_candidates = [\n",
        "        {'Recived Amount': 'raw_date', 'Unnamed: 1': 'amount', 'Expance': 'description', 'Unnamed: 3': 'note'},\n",
        "        {'recived_amount': 'raw_date', 'unnamed:_1': 'amount', 'expance': 'description', 'unnamed:_3': 'note'}\n",
        "    ]\n",
        "    renamed = False\n",
        "    for m in rename_map_candidates:\n",
        "        if set(m.keys()).issubset(expense.columns):\n",
        "            expense = expense.rename(columns=m); renamed = True; break\n",
        "    if not renamed:\n",
        "        # Fallback: normalize cols and try again\n",
        "        expense = normalize_columns(expense)\n",
        "        for m in rename_map_candidates:\n",
        "            if set(m.keys()).issubset(expense.columns):\n",
        "                expense = expense.rename(columns=m); renamed = True; break\n",
        "        # If it was already clean post-normalization, this will pass:\n",
        "        already_clean = {'raw_date','amount','description','note'}.issubset(expense.columns)\n",
        "        if not (renamed or already_clean):\n",
        "            raise ValueError(\"Could not map Expense headers to ['raw_date','amount','description','note'].\")\n",
        "\n",
        "print(\"Expense headers AFTER handling:\", expense.columns.tolist())\n",
        "\n",
        "# Ensure date/amount types; reuse existing date/year/month if present\n",
        "if 'date' not in expense.columns:\n",
        "    expense['date'] = pd.to_datetime(expense['raw_date'], errors='coerce')\n",
        "else:\n",
        "    expense['date'] = pd.to_datetime(expense['date'], errors='coerce')\n",
        "\n",
        "expense['amount'] = pd.to_numeric(expense['amount'], errors='coerce')\n",
        "\n",
        "# Keep valid rows\n",
        "expense = expense[(expense['date'].notna()) & (expense['amount'].notna())].copy()\n",
        "\n",
        "# Ensure year/month\n",
        "if 'year' not in expense.columns:\n",
        "    expense['year'] = expense['date'].dt.year\n",
        "if 'month' not in expense.columns:\n",
        "    expense['month'] = expense['date'].dt.month\n",
        "\n",
        "# Aggregate monthly totals (idempotent)\n",
        "monthly_expense = (\n",
        "    expense.groupby(['year','month'], as_index=False)['amount']\n",
        "           .sum()\n",
        "           .rename(columns={'amount':'monthly_expense'})\n",
        ")\n",
        "\n",
        "print(\"Monthly expense preview:\\n\", monthly_expense.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxHH9uHuU0ZG",
        "outputId": "7b87c455-11e1-4b18-f7db-c33b130bc35f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expense headers BEFORE handling: ['Recived Amount', 'Unnamed: 1', 'Expance', 'Unnamed: 3']\n",
            "Expense headers AFTER handling: ['raw_date', 'amount', 'description', 'note']\n",
            "Monthly expense preview:\n",
            "    year  month  monthly_expense\n",
            "0  2022      6           5000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 6 ‚Äî Merge expense into sales"
      ],
      "metadata": {
        "id": "LH7DQKs5bR_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guard: need year/month in sales\n",
        "if not {'year','month'}.issubset(merged.columns):\n",
        "    raise ValueError(\"Merged dataset lacks 'year'/'month'. Ensure STEP 3 created them from a 'date' column.\")\n",
        "\n",
        "merged = merged.merge(monthly_expense, on=['year','month'], how='left')\n",
        "merged['monthly_expense'] = merged['monthly_expense'].fillna(0.0)\n",
        "\n",
        "# expense per unit\n",
        "if {'year','month'}.issubset(merged.columns):\n",
        "    month_qty = merged.groupby(['year','month'])['qty' if 'qty' in merged.columns else 'pcs'].sum().rename('month_total_qty').reset_index()\n",
        "    merged = merged.merge(month_qty, on=['year','month'], how='left')\n",
        "    merged['expense_per_unit_month'] = np.where(\n",
        "        merged['month_total_qty'] > 0,\n",
        "        merged['monthly_expense'] / merged['month_total_qty'],\n",
        "        np.nan)\n",
        "for q in ['qty','pcs']:\n",
        "    if q in merged.columns:\n",
        "        merged[q] = pd.to_numeric(merged[q], errors='coerce')\n",
        "\n",
        "# Sort by date if present\n",
        "if 'date' in merged.columns:\n",
        "    merged = merged.sort_values('date')\n",
        "\n",
        "# Save\n",
        "merged.to_csv('/content/amazon_with_expense_clean.csv', index=False)\n",
        "print(\"üíæ Saved final cleaned dataset: /content/amazon_with_expense_clean.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3HWeinybSXs",
        "outputId": "30d6d75a-c1a0-4035-a036-a440580cda16"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved final cleaned dataset: /content/amazon_with_expense_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 7 ‚Äî Verify merge coverage"
      ],
      "metadata": {
        "id": "wyuBUbdLPc5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify expense coverage\n",
        "print(\"\\nUnique year-month combinations in sales data:\")\n",
        "print(merged[['year','month']].drop_duplicates().sort_values(['year','month']))\n",
        "\n",
        "print(\"\\nUnique year-month combinations in expense data:\")\n",
        "print(monthly_expense[['year','month']].drop_duplicates().sort_values(['year','month']))\n",
        "\n",
        "print(\"\\nMonthly expense statistics:\")\n",
        "print(merged['monthly_expense'].describe())\n",
        "\n",
        "zero_expense = (merged['monthly_expense'] == 0).mean() * 100\n",
        "print(f\"\\n‚ö†Ô∏è {zero_expense:.2f}% of rows have monthly_expense = 0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRIDk1xPPZ5f",
        "outputId": "d7ccacdd-8568-4032-de95-817a5b2a5fc6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique year-month combinations in sales data:\n",
            "          year  month\n",
            "128989  2021.0    6.0\n",
            "129708  2021.0    7.0\n",
            "131077  2021.0    8.0\n",
            "132018  2021.0    9.0\n",
            "139220  2021.0   10.0\n",
            "135034  2021.0   11.0\n",
            "141634  2021.0   12.0\n",
            "136485  2022.0    1.0\n",
            "142742  2022.0    2.0\n",
            "144421  2022.0    3.0\n",
            "48246   2022.0    4.0\n",
            "89793   2022.0    5.0\n",
            "127869  2022.0    6.0\n",
            "\n",
            "Unique year-month combinations in expense data:\n",
            "   year  month\n",
            "0  2022      6\n",
            "\n",
            "Monthly expense statistics:\n",
            "count    177008.000000\n",
            "mean       1064.838877\n",
            "std        2047.031076\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000000\n",
            "max        5000.000000\n",
            "Name: monthly_expense, dtype: float64\n",
            "\n",
            "‚ö†Ô∏è 78.70% of rows have monthly_expense = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 8 ‚Äî Quick sanity checks"
      ],
      "metadata": {
        "id": "8UZNGmF5IwiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rows:\", len(merged))\n",
        "if 'monthly_expense' in merged.columns:\n",
        "    print(\"Monthly expense stats:\\n\", merged['monthly_expense'].describe())\n",
        "print(\"Sample columns:\\n\", list(merged.columns)[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA9AAsAGIw_P",
        "outputId": "a01e304f-2a2e-45a9-db26-79515db774ef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 177008\n",
            "Monthly expense stats:\n",
            " count    177008.000000\n",
            "mean       1064.838877\n",
            "std        2047.031076\n",
            "min           0.000000\n",
            "25%           0.000000\n",
            "50%           0.000000\n",
            "75%           0.000000\n",
            "max        5000.000000\n",
            "Name: monthly_expense, dtype: float64\n",
            "Sample columns:\n",
            " ['index', 'order_id', 'date', 'status', 'fulfilment', 'sales_channel', 'ship-service-level', 'style', 'sku', 'category', 'size', 'asin', 'courier_status', 'qty', 'currency', 'amount', 'ship-city', 'ship-state', 'ship-postal-code', 'ship-country', 'promotion-ids', 'b2b', 'fulfilled-by', 'unnamed:_22', 'months', 'customer', 'pcs', 'rate', 'gross_amt', 'style_id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 9 ‚Äî Free-text detection"
      ],
      "metadata": {
        "id": "INChHg6objLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_cols = merged.select_dtypes(include='object').columns.tolist()\n",
        "free_text_cols, structured_text_cols = [], []\n",
        "n_rows = len(merged)\n",
        "\n",
        "for col in obj_cols:\n",
        "    avg_len = merged[col].astype(str).str.len().mean()\n",
        "    uniq_ratio = merged[col].nunique(dropna=True) / max(n_rows, 1)\n",
        "    # Heuristics: long strings or highly unique ‚Üí free text\n",
        "    if (avg_len >= 25) or (uniq_ratio >= 0.50):\n",
        "        free_text_cols.append(col)\n",
        "    else:\n",
        "        structured_text_cols.append(col)\n",
        "\n",
        "print(\"\\nüîé Free-text candidates:\", free_text_cols)\n",
        "\n",
        "# Save free-text subset for later NLP feature engineering (do not drop from merged unless you want)\n",
        "if free_text_cols:\n",
        "    merged[free_text_cols].to_pickle('/content/free_text_columns.pkl')\n",
        "    print(\"üíæ Saved free-text columns to /content/free_text_columns.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ-Yo-wtbbkY",
        "outputId": "6541e30e-5d91-460c-ec73-927e7cddf561"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Free-text candidates: ['order_id', 'promotion-ids']\n",
            "üíæ Saved free-text columns to /content/free_text_columns.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check missing values before eda"
      ],
      "metadata": {
        "id": "1MxTBjoBTp_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîé Checking missing values before EDA...\\n\")\n",
        "\n",
        "# 1. Calculate missing values in %\n",
        "missing_percent = merged.isnull().mean() * 100\n",
        "print(\"üìä Missing values per column (%):\")\n",
        "print(missing_percent.sort_values(ascending=False))\n",
        "\n",
        "# 2. Drop 'pcs' if it's completely missing\n",
        "if 'pcs' in merged.columns and merged['pcs'].isna().all():\n",
        "    merged.drop(columns=['pcs'], inplace=True)\n",
        "    print(\"\\nüßπ Dropped 'pcs' column (100% missing).\")\n",
        "\n",
        "# 3. Recalculate global missing value rate\n",
        "total_missing = merged.isnull().sum().sum()\n",
        "total_cells = merged.shape[0] * merged.shape[1]\n",
        "missing_percent_total = (total_missing / total_cells) * 100\n",
        "\n",
        "print(f\"\\n‚úÖ Total missing values after cleanup: {total_missing:,} \"\n",
        "      f\"({missing_percent_total:.2f}% of all data points)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvgMqbhcTt5H",
        "outputId": "0b955c62-1851-41a8-852b-1e253b321c53"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Checking missing values before EDA...\n",
            "\n",
            "üìä Missing values per column (%):\n",
            "pcs                       100.0\n",
            "index                       0.0\n",
            "date                        0.0\n",
            "status                      0.0\n",
            "fulfilment                  0.0\n",
            "order_id                    0.0\n",
            "ship-service-level          0.0\n",
            "style                       0.0\n",
            "sku                         0.0\n",
            "category                    0.0\n",
            "size                        0.0\n",
            "asin                        0.0\n",
            "courier_status              0.0\n",
            "qty                         0.0\n",
            "currency                    0.0\n",
            "amount                      0.0\n",
            "ship-city                   0.0\n",
            "ship-state                  0.0\n",
            "ship-postal-code            0.0\n",
            "ship-country                0.0\n",
            "promotion-ids               0.0\n",
            "sales_channel               0.0\n",
            "b2b                         0.0\n",
            "fulfilled-by                0.0\n",
            "months                      0.0\n",
            "unnamed:_22                 0.0\n",
            "customer                    0.0\n",
            "rate                        0.0\n",
            "gross_amt                   0.0\n",
            "style_id                    0.0\n",
            "catalog                     0.0\n",
            "weight                      0.0\n",
            "tp                          0.0\n",
            "mrp_old                     0.0\n",
            "final_mrp_old               0.0\n",
            "ajio_mrp                    0.0\n",
            "amazon_mrp                  0.0\n",
            "amazon_fba_mrp              0.0\n",
            "flipkart_mrp                0.0\n",
            "limeroad_mrp                0.0\n",
            "myntra_mrp                  0.0\n",
            "paytm_mrp                   0.0\n",
            "snapdeal_mrp                0.0\n",
            "sku_code                    0.0\n",
            "design_no.                  0.0\n",
            "stock                       0.0\n",
            "color                       0.0\n",
            "year                        0.0\n",
            "month                       0.0\n",
            "weekday                     0.0\n",
            "is_weekend                  0.0\n",
            "monthly_expense             0.0\n",
            "month_total_qty             0.0\n",
            "expense_per_unit_month      0.0\n",
            "dtype: float64\n",
            "\n",
            "üßπ Dropped 'pcs' column (100% missing).\n",
            "\n",
            "‚úÖ Total missing values after cleanup: 0 (0.00% of all data points)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check for excluded files (cloud and p&l)"
      ],
      "metadata": {
        "id": "0VLSMRocQ8Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cloud file: show that it‚Äôs a provider comparison table, not sales data\n",
        "try:\n",
        "    cloud = pd.read_csv('/content/Cloud Warehouse Compersion Chart.csv')\n",
        "    print(\"Cloud shape:\", cloud.shape)\n",
        "    print(\"Cloud columns:\", cloud.columns.tolist())\n",
        "    print(\"\\nCloud head (3 rows):\\n\", cloud.head(3).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Cloud file not found or unreadable:\", e)\n",
        "\n",
        "#  P&L file: quick SKU mismatch evidence\n",
        "try:\n",
        "    pl = pd.read_csv('/content/P  L March 2021.csv')\n",
        "    print(\"\\nP&L shape:\", pl.shape)\n",
        "    print(\"P&L columns (first 12):\", list(pl.columns)[:12])\n",
        "\n",
        "    # overlap with merged files:\n",
        "    if 'merged' in globals() and 'sku' in merged.columns and 'sku' in pl.columns:\n",
        "        sales_skus = set(merged['sku'].astype(str).str.strip().str.lower().dropna().unique())\n",
        "        pl_skus    = set(pl['sku'].astype(str).str.strip().str.lower().dropna().unique())\n",
        "        overlap = len(sales_skus & pl_skus)\n",
        "        print(f\"SKU overlap (sales vs P&L): {overlap} common SKUs \"\n",
        "              f\"out of {len(sales_skus)} (sales) and {len(pl_skus)} (P&L)\")\n",
        "    else:\n",
        "        print(\"SKU overlap not computed (missing 'merged' or 'sku' column).\")\n",
        "except Exception as e:\n",
        "    print(\"P&L file not found or unreadable:\", e)\n",
        "\n",
        "# Clean\n",
        "for _v in ['cloud','pl','sales_skus','pl_skus','overlap']:\n",
        "    if _v in globals(): del globals()[_v]\n",
        "print(\"\\n check completed (no changes to the modeling dataset).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBgzx8szQ6gd",
        "outputId": "3f321bfe-1cd6-4452-e092-681406c4086e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloud shape: (50, 4)\n",
            "Cloud columns: ['index', 'Shiprocket', 'Unnamed: 1', 'INCREFF']\n",
            "\n",
            "Cloud head (3 rows):\n",
            "  index                    Shiprocket       Unnamed: 1          INCREFF\n",
            "     0                         Heads Price (Per Unit) Price (Per Unit)\n",
            "     1 Inbound (Fresh Stock and RTO)            ‚Çπ4.00                4\n",
            "     2                      Outbound            ‚Çπ7.00               11\n",
            "\n",
            "P&L shape: (1330, 18)\n",
            "P&L columns (first 12): ['index', 'Sku', 'Style Id', 'Catalog', 'Category', 'Weight', 'TP 1', 'TP 2', 'MRP Old', 'Final MRP Old', 'Ajio MRP', 'Amazon MRP']\n",
            "SKU overlap not computed (missing 'merged' or 'sku' column).\n",
            "\n",
            " check completed (no changes to the modeling dataset).\n"
          ]
        }
      ]
    }
  ]
}